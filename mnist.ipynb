{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download mnist_dataset to desired directory and split to train, val, test\n",
    "from make_modular.configs import TRAIN_DATASET_PATH, TEST_DATASET_PATH\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_ds = MNIST(TRAIN_DATASET_PATH, train=True, download=True, transform=transform)\n",
    "test_ds = MNIST(TEST_DATASET_PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "# split train_dataset to train and validation dataset\n",
    "val_percent = .15\n",
    "val_size = int(val_percent * len(train_ds))\n",
    "train_size = len(train_ds) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51000, 9000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our datasets\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataLoaders \n",
    "from make_modular.configs import batch_size\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloaders\n",
    "images, labels = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image_tesnsor: (b, c, h, w) --> numpy_array: (b, h, w, c)=(b, 28, 28, 1)\n",
    "# img = images.numpy().transpose(0, 2, 3, 1)\n",
    "# plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images of dataset\n",
    "from make_modular.utils import show_images\n",
    "# show_images(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from make_modular.model import MNIST_MODEL_1\n",
    "# from make_modular.utils import print_train_time\n",
    "\n",
    "# # find best lr\n",
    "# from make_modular.utils import find_best_lr\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# n_iter_lr_find = 3\n",
    "# lr_exp_list = find_best_lr(MNIST_MODEL_1, n_iter=n_iter_lr_find, train_dl=train_dl, val_dl=val_dl)\n",
    "# end_time = time.time()\n",
    "# total_time = print_train_time(start_time, end_time)\n",
    "\n",
    "# best_lr = min(lr_exp_list, key=lambda x: x[list(x.keys())[0]]['val_loss'][-1])\n",
    "# # print(f'best_lr : {best_lr}')\n",
    "\n",
    "# lrs = [] # key of dict:the lr\n",
    "# losses_correspond_lr = [] # the test_loss with correspond lr\n",
    "\n",
    "# for dictionary in lr_exp_list:\n",
    "#     min_test_loss_key = min(dictionary, key=lambda k: dictionary[k]['val_loss'][-1])\n",
    "#     lrs.append(min_test_loss_key)\n",
    "#     losses_correspond_lr.append(dictionary[min_test_loss_key]['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_lr =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d76cb6850354a0ca5f9e87fe47e0147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888957186, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from make_modular.engine import train\n",
    "from make_modular.configs import device\n",
    "from make_modular.model import MNIST_MODEL_1\n",
    "from make_modular.utils import print_train_time\n",
    "\n",
    "model = MNIST_MODEL_1()\n",
    "\n",
    "lr = 5e-2\n",
    "epochs = 1\n",
    "num_exp = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# configs to save\n",
    "hp_configs = {\n",
    "    'model':model.__class__.__name__,\n",
    "    'lr':lr,\n",
    "    'epochs':epochs,\n",
    "    'device':device\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "results = train(model=model,\n",
    "                train_dl=train_dl,\n",
    "                val_dl=val_dl,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                device=device,\n",
    "                epochs=epochs,\n",
    "                save_wandb=True,\n",
    "                num_exp=num_exp,\n",
    "                hyper_param_config=hp_configs)\n",
    "\n",
    "final_res_lr.append({lr:results})\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = print_train_time(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNIST_MODEL_1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from make_modular.utils import plot_loss_curves\n",
    "# plot_loss_curves(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "root_model_path = './models/'\n",
    "model_name = 'mn_model_1.pth'\n",
    "# # model_name = 'model_2.pth'\n",
    "# # model_name = 'model_3.pth'\n",
    "# # model_name = 'model_4.pth'\n",
    "# # model_name = 'model_5.pth'\n",
    "# # model_name = 'model_6.pth'\n",
    "model_path = root_model_path + model_name\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = MNIST_MODEL_1()\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04948953012743028, 0.9841759554140127, 0.9817392659722036)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model on test set\n",
    "from make_modular.engine import test_one_epoch\n",
    "test_loss, test_acc, test_f1_score = test_one_epoch(model=model,\n",
    "                                                    val_dl=test_dl,\n",
    "                                                    loss_fn=loss_fn,\n",
    "                                                    device=device)\n",
    "test_loss, test_acc, test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best lr for models\n",
    "# write function to dataloaders\n",
    "# there is a problem with find best lr model should be inside the method \n",
    "# ask from gpt how to create new model in each for iterate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

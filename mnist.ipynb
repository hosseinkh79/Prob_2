{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download mnist_dataset to desired directory and split to train, val, test\n",
    "from make_modular.configs import TRAIN_DATASET_PATH, TEST_DATASET_PATH\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_ds = MNIST(TRAIN_DATASET_PATH, train=True, download=True, transform=transform)\n",
    "test_ds = MNIST(TEST_DATASET_PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "# split train_dataset to train and validation dataset\n",
    "val_percent = .15\n",
    "val_size = int(val_percent * len(train_ds))\n",
    "train_size = len(train_ds) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51000, 9000, 10000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our datasets\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataLoaders \n",
    "from make_modular.configs import batch_size\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloaders\n",
    "images, labels = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image_tesnsor: (b, c, h, w) --> numpy_array: (b, h, w, c)=(b, 28, 28, 1)\n",
    "# img = images.numpy().transpose(0, 2, 3, 1)\n",
    "# plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images of dataset\n",
    "from make_modular.utils import show_images\n",
    "# show_images(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class MODEL_1(nn.Module):\n",
    "    def __init__(self, in_channel=1, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # W_out ((w_in - kernel_size + 2p)/stride) +1\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=32, kernel_size=3), # (b, 1, 28, 28) --> (b, 32, 26, 26)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3), # (b, 32, 26, 26) --> (b, 64, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2), # (b, 64, 24, 24) --> (b, 64, 12, 12)\n",
    "            nn.Dropout(.5),\n",
    "            nn.Flatten() # (b, 64, 12, 12) --> (b, 64*12*12)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*12*12, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "        # self.fc = nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "model = MODEL_1()\n",
    "input = torch.randn(4, 1, 28, 28)\n",
    "output = model(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:2.29 | val_loss:2.27 | train_acc:0.16 | val_acc:0.22 | train_f1_score:0.13 | val_f1_score:0.20\n"
     ]
    }
   ],
   "source": [
    "from make_modular.engine import train\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MODEL_1()\n",
    "\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adadelta(params=model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dl=train_dl,\n",
    "                val_dl=val_dl,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                device=device,\n",
    "                epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "# use data augmentation\n",
    "# plot loss curves\n",
    "# draw some of models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
